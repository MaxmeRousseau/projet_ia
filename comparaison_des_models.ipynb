{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "\n",
    "# Détection du device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Chemins vers les répertoires des modèles locaux (présents dans le workspace)\n",
    "model1_dir = 'humor_detection_model01'\n",
    "model2_dir = 'humor_model_multilingual'\n",
    "\n",
    "# Chargement des tokenizers et modèles (sequence classification)\n",
    "# from_pretrained acceptera le dossier contenant config.json et les poids (model.safetensors)\n",
    "try:\n",
    "    tokenizer1 = AutoTokenizer.from_pretrained(model1_dir)\n",
    "    model1 = AutoModelForSequenceClassification.from_pretrained(model1_dir).to(device)\n",
    "    print('Loaded model1 from', model1_dir)\n",
    "except Exception as e:\n",
    "    print('Erreur lors du chargement du model1:', e)\n",
    "    print('Vérifiez que le dossier', model1_dir, 'contient config.json et model.safetensors ou les fichiers de poids attendus.')\n",
    "\n",
    "try:\n",
    "    tokenizer2 = AutoTokenizer.from_pretrained(model2_dir)\n",
    "    model2 = AutoModelForSequenceClassification.from_pretrained(model2_dir).to(device)\n",
    "    print('Loaded model2 from', model2_dir)\n",
    "except Exception as e:\n",
    "    print('Erreur lors du chargement du model2:', e)\n",
    "    print('Vérifiez que le dossier', model2_dir, 'contient config.json et model.safetensors ou les fichiers de poids attendus.')\n",
    "\n",
    "# Fonction d'inférence simple: renvoie la classe prédite et les probabilités\n",
    "import torch.nn.functional as F\n",
    "def predict(texts, tokenizer, model, device, return_probs=True):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "    if return_probs:\n",
    "        return preds, probs\n",
    "    return preds\n",
    "\n",
    "examples = [\n",
    "    \"I told my computer I needed a break, and it said no problem — it crashed.\",\n",
    "    \"Why don't scientists trust atoms? Because they make up everything!\",\n",
    "    \"This is a test sentence.\",\n",
    "    \"Ce texte est sérieux et pas drôle.\",\n",
    "    \"Pourquoi les plongeurs plongent-ils toujours en arrière et jamais en avant ? Parce que sinon ils tombent dans le bateau.\"\n",
    "]\n",
    "\n",
    "if 'model1' in globals() and 'tokenizer1' in globals():\n",
    "    preds1, probs1 = predict(examples, tokenizer1, model1, device)\n",
    "    print('\\nResults for model1 (humor_detection_model01):')\n",
    "    for t, p, prob in zip(examples, preds1, probs1):\n",
    "        print(f'Example: {t}')\n",
    "        print(f'  Predicted class: {p} | probs: {prob}')\n",
    "\n",
    "if 'model2' in globals() and 'tokenizer2' in globals():\n",
    "    preds2, probs2 = predict(examples, tokenizer2, model2, device)\n",
    "    print('\\nResults for model2 (humor_model_multilingual):')\n",
    "    for t, p, prob in zip(examples, preds2, probs2):\n",
    "        print(f'Example: {t}')\n",
    "        print(f'  Predicted class: {p} | probs: {prob}')\n",
    "\n",
    "# Note: les indices de classes (0/1/...) dépendent de la configuration du modèle.\n",
    "# Si vous avez des étiquettes textuelles, mappez-les avec model.config.id2label\n",
    "if 'model1' in globals():\n",
    "    try:\n",
    "        print('\\nModel1 id2label:', model1.config.id2label)\n",
    "    except Exception:\n",
    "        pass\n",
    "if 'model2' in globals():\n",
    "    try:\n",
    "        print('Model2 id2label:', model2.config.id2label)\n",
    "    except Exception:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
