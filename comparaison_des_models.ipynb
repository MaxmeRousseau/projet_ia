{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "\n",
    "# Détection du device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Chemins vers les répertoires des modèles locaux (présents dans le workspace)\n",
    "model1_dir = 'humor_detection_model01'\n",
    "model2_dir = 'humor_model_multilingual'\n",
    "\n",
    "def load_tokenizer_with_fallback(model_dir, fallback_name=None):\n",
    "    try:\n",
    "        print(f\"Trying to load tokenizer from local dir: {model_dir} (local_files_only=True)\")\n",
    "        return AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "    except Exception as e:\n",
    "        print('Local tokenizer load failed:', e)\n",
    "        if fallback_name is not None:\n",
    "            try:\n",
    "                print('Trying fallback tokenizer from Hugging Face:', fallback_name)\n",
    "                return AutoTokenizer.from_pretrained(fallback_name, local_files_only=False)\n",
    "            except Exception as e2:\n",
    "                print('Fallback tokenizer failed:', e2)\n",
    "        print('No tokenizer available for', model_dir)\n",
    "        return None\n",
    "\n",
    "def load_model_with_fallback(model_dir):\n",
    "    try:\n",
    "        print(f\"Trying to load model from local dir: {model_dir} (local_files_only=True)\")\n",
    "        return AutoModelForSequenceClassification.from_pretrained(model_dir, local_files_only=True).to(device)\n",
    "    except Exception as e:\n",
    "        print('Local model load failed:', e)\n",
    "        # If a safetensors file exists, try to load it with safetensors.torch\n",
    "        safetensors_path = os.path.join(model_dir, 'model.safetensors')\n",
    "        if os.path.exists(safetensors_path):\n",
    "            try:\n",
    "                print('Found model.safetensors — attempting safetensors load')\n",
    "                # load config and build model skeleton\n",
    "                cfg = AutoConfig.from_pretrained(model_dir, local_files_only=True)\n",
    "                model = AutoModelForSequenceClassification.from_config(cfg).to(device)\n",
    "                try:\n",
    "                    from safetensors.torch import load_file as safe_load\n",
    "                except Exception as e2:\n",
    "                    print('safetensors package not installed. Install with: %pip install safetensors')\n",
    "                    raise e2\n",
    "                state = safe_load(safetensors_path, device=device)\n",
    "                # state is a dict of tensors; load into model (allow strict=False for missing keys)\n",
    "                model.load_state_dict(state, strict=False)\n",
    "                print('Loaded weights from safetensors into model object')\n",
    "                return model\n",
    "            except Exception as e3:\n",
    "                print('Failed to load model from safetensors:', e3)\n",
    "        print('No usable model found in', model_dir)\n",
    "        return None\n",
    "\n",
    "# Choisir des fallback tokenizers raisonnables\n",
    "# model1 semble avoir vocab_size 30522 -> distilbert-base-uncased est un bon candidat\n",
    "# model2 a un vocab_size plus grand -> utiliser distilbert-base-multilingual-cased comme fallback\n",
    "fallback_tokenizer1 = 'distilbert-base-uncased'\n",
    "fallback_tokenizer2 = 'distilbert-base-multilingual-cased'\n",
    "\n",
    "# Charger tokenizers et modèles avec stratégies de repli\n",
    "tokenizer1 = load_tokenizer_with_fallback(model1_dir, fallback_tokenizer1)\n",
    "model1 = load_model_with_fallback(model1_dir)\n",
    "if model1 is None:\n",
    "    print('model1 not loaded — vérifiez le contenu du dossier ou installez safetensors si nécessaire')\n",
    "tokenizer2 = load_tokenizer_with_fallback(model2_dir, fallback_tokenizer2)\n",
    "model2 = load_model_with_fallback(model2_dir)\n",
    "if model2 is None:\n",
    "    print('model2 not loaded — vérifiez le contenu du dossier ou installez safetensors si nécessaire')\n",
    "\n",
    "# Fonction d'inférence simple: renvoie la classe prédite et les probabilités\n",
    "import torch.nn.functional as F\n",
    "def predict(texts, tokenizer, model, device, return_probs=True):\n",
    "    if tokenizer is None or model is None:\n",
    "        raise ValueError('tokenizer et model doivent être fournis')\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "    if return_probs:\n",
    "        return preds, probs\n",
    "    return preds\n",
    "\n",
    "# Quelques exemples de test\n",
    "examples = [\n",
    "    \"I told my computer I needed a break, and it said no problem — it crashed.\",\n",
    "    \"Why don't scientists trust atoms? Because they make up everything!\",\n",
    "    \"This is a test sentence.\",\n",
    "    \"Ce texte est sérieux et pas drôle.\",\n",
    "    \"Pourquoi les plongeurs plongent-ils toujours en arrière et jamais en avant ? Parce que sinon ils tombent dans le bateau.\"\n",
    " ]\n",
    "\n",
    "# Exécuter les tests si possible\n",
    "if tokenizer1 is not None and model1 is not None:\n",
    "    try:\n",
    "        preds1, probs1 = predict(examples, tokenizer1, model1, device)\n",
    "        print('\\nResults for model1 (humor_detection_model01):')\n",
    "        for t, p, prob in zip(examples, preds1, probs1):\n",
    "            print(f'Example: {t}')\n",
    "            print(f'  Predicted class: {p} | probs: {prob}')\n",
    "    except Exception as e:\n",
    "        print('Erreur lors de la prédiction avec model1:', e)\n",
    "\n",
    "if tokenizer2 is not None and model2 is not None:\n",
    "    try:\n",
    "        preds2, probs2 = predict(examples, tokenizer2, model2, device)\n",
    "        print('\\nResults for model2 (humor_model_multilingual):')\n",
    "        for t, p, prob in zip(examples, preds2, probs2):\n",
    "            print(f'Example: {t}')\n",
    "            print(f'  Predicted class: {p} | probs: {prob}')\n",
    "    except Exception as e:\n",
    "        print('Erreur lors de la prédiction avec model2:', e)\n",
    "\n",
    "# Afficher id2label si disponible\n",
    "if model1 is not None:\n",
    "    try:\n",
    "        print('\\nModel1 id2label:', model1.config.id2label)\n",
    "    except Exception:\n",
    "        pass\n",
    "if model2 is not None:\n",
    "    try:\n",
    "        print('Model2 id2label:', model2.config.id2label)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62074c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des deux modèles: chargement avec from_pretrained et prédictions sur exemples fournis\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "texts = [\n",
    "    \"j'ai faim\",\n",
    "    \"Quelle mamie fait peur aux voleurs ? Mamie Traillette.\",\n",
    "    \"Pourquoi les plongeurs plongent-ils toujours en arrière et jamais en avant ? Parce que sinon ils tombent dans le bateau.\",\n",
    "    \"Quel est le comble pour un électricien ? De ne pas être au courant.\",\n",
    "    \"Pourquoi les squelettes ne se battent-ils jamais entre eux ? Ils n'ont pas le cran.\",\n",
    "    \"소 잃고 외양간 고친다\",\n",
    "    \"백지장도 맞들면 낫다\",\n",
    "    \"¿Por qué los pájaros no usan Facebook? Porque ya tienen Twitter.\",\n",
    " ]\n",
    "\n",
    "def run_simple_pipeline(model_dir, tokenizer):\n",
    "    print(f\"\\n--- Running pipeline for {model_dir} ---\")\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "        inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "        inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        for text, pred in zip(texts, predictions):\n",
    "            label = 'Humor' if pred.item() == 1 else 'Not Humor'\n",
    "            print(f\"Text: {text}\\nPrediction: {label}\\n\")\n",
    "    except Exception as e:\n",
    "        print('Erreur pipeline pour', model_dir, ':', e)\n",
    "\n",
    "# Exécuter la pipeline pour model1 et model2 (utiliser tokenizer1/tokenizer2 si disponibles)\n",
    "if tokenizer1 is not None:\n",
    "    run_simple_pipeline('humor_detection_model01', tokenizer1)\n",
    "else:\n",
    "    print('tokenizer1 non disponible — impossible d exécuter pipeline pour model1')\n",
    "if tokenizer2 is not None:\n",
    "    run_simple_pipeline('humor_model_multilingual', tokenizer2)\n",
    "else:\n",
    "    print('tokenizer2 non disponible — impossible d exécuter pipeline pour model2')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
