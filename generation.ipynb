{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24f380",
   "metadata": {},
   "source": [
    "# Génération de blagues en français\n",
    "Ce notebook montre plusieurs approches pour générer des blagues en français en partant de modèles que vous avez déjà entraînés pour détecter l'humour.\n",
    "\n",
    "Approches recommandées :\n",
    "- Prompt engineering : utiliser un modèle causal pré-entraîné (idéalement en français ou multilingue) et concevoir des prompts qui guident la génération.\n",
    "- Filtrage / reranking : générer plusieurs candidats puis utiliser vos modèles de détection (`humor_detection_model01`, `humor_model_multilingual`) pour scorer et garder les meilleures blagues.\n",
    "- Fine-tuning : si vous avez un dataset de blagues (`datasets/shortjokes.csv`), fine-tuner un modèle causal sur ce corpus donne généralement de meilleurs résultats stylistiques et linguistiques.\n",
    "\n",
    "Dans les cellules suivantes :\n",
    "1) Exemple de génération avec `transformers` (pipeline text-generation)\n",
    "2) Exemple de filtrage / reranking en chargeant vos classifieurs locaux\n",
    "3) Squelette pour fine-tuning si vous souhaitez entraîner un modèle génératif sur vos blagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84921af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: génération de blagues en français\n",
    "# Pré-requis: transformers, torch, datasets installés (voir la cellule d'en haut)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Choix du modèle: ici on utilise un modèle causal multilingue léger si disponible.\n",
    "# Remplacez 'gpt2' par le chemin vers un modèle local (ex: './humor_model_multilingual') ou HF id.\n",
    "model_name_or_path = 'gpt2'  # remplacer par un modèle FR ou local si vous en avez\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "# GPT-2 n'a pas de token pad par défaut, définir pad_token si absent\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "if device == 0:\n",
    "    model = model.to('cuda')\n",
    "\n",
    "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Exemple de fonction de génération simple en français\n",
    "def generate_jokes(prompt, max_length=60, temperature=1.0, top_p=0.9, num_return_sequences=5):\n",
    "    full_prompt = prompt if prompt.endswith('\\n') else prompt + '\\n'\n",
    "    outputs = gen(full_prompt, max_length=max_length, do_sample=True, temperature=temperature, top_p=top_p, num_return_sequences=num_return_sequences)\n",
    "    return [o['generated_text'][len(full_prompt):].strip() for o in outputs]\n",
    "\n",
    "# Exemples d'appels\n",
    "prompt = 'Génère une blague courte en français :'\n",
    "jokes = generate_jokes(prompt, max_length=80, temperature=0.9, top_p=0.95, num_return_sequences=8)\n",
    "for i,j in enumerate(jokes,1):\n",
    "    print(f'--- Blague {i} ---')\n",
    "    print(j)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage / reranking avec vos modèles de détection d'humour locaux\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Chemins vers vos modèles locaux (existant dans le repo)\n",
    "local_model_1 = './humor_detection_model01'\n",
    "local_model_2 = './humor_model_multilingual'\n",
    "\n",
    "def load_classifier(path):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(path)\n",
    "        mdl = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, device=device)\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        print(f'Erreur chargement {path}:', e)\n",
    "        return None\n",
    "\n",
    "clf1 = load_classifier(local_model_1)\n",
    "clf2 = load_classifier(local_model_2)\n",
    "\n",
    "def score_jokes(jokes, clf):\n",
    "    # Renvoie la probabilité 'humour' si le modèle a deux classes [not_humor, humor] ou cherche la classe la plus probable\n",
    "    scores = []\n",
    "    if clf is None:\n",
    "        return [0.0]*len(jokes)\n",
    "    for j in jokes:\n",
    "        res = clf(j)  # return_all_scores=True -> list of dicts per class\n",
    "        # Res est une liste (pour batch de 1) contenant une liste de scores par classe\n",
    "        try:\n",
    "            class_scores = res[0]\n",
    "            # Chercher la classe contenant 'humor' ou prendre la plus élevée\n",
    "            # On renvoie la probabilité max comme proxy\n",
    "            probs = {c['label']: c['score'] for c in class_scores}\n",
    "            # heuristique: si 'humor' ou 'HUMOR' présent, l'utiliser, sinon max score\n",
    "            h = None\n",
    "            for key in probs:\n",
    "                if 'humor' in key.lower() or 'humour' in key.lower():\n",
    "                    h = probs[key]\n",
    "                    break\n",
    "            if h is None:\n",
    "                h = max(probs.values())\n",
    "        except Exception as e:\n",
    "            h = 0.0\n",
    "        scores.append(float(h))\n",
    "    return scores\n",
    "\n",
    "# Score avec les deux classifieurs et moyenne des probabilités\n",
    "scores1 = score_jokes(jokes, clf1)\n",
    "scores2 = score_jokes(jokes, clf2)\n",
    "combined = [(s1 + s2) / 2.0 for s1, s2 in zip(scores1, scores2)]\n",
    "\n",
    "# Reranker et afficher\n",
    "order = np.argsort(combined)[::-1]  # décroissant\n",
    "print('Reranking des blagues par score moyen de humour :')\n",
    "for idx in order:\n",
    "    print('--- Blague (score=', round(combined[idx],3),') ---')\n",
    "    print(jokes[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935dc781",
   "metadata": {},
   "source": [
    "## Fine-tuning minimal (squelette)\n",
    "Si vous souhaitez fine-tuner un modèle causal sur `datasets/shortjokes.csv` :\n",
    "1. Charger le CSV et construire un dataset texte (une blague par exemple).\n",
    "2. Tokenizer et préparer les exemples pour le modèle causal (concatenate prompt+target si besoin).\n",
    "3. Utiliser `transformers.Trainer` ou `accelerate` pour l'entraînement.\n",
    "\n",
    "Exemple minimal :\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "ds = load_dataset('csv', data_files='datasets/shortjokes.csv')['train']\n",
    "# Supposons que la colonne s'appelle 'text'\n",
    "ds = ds.map(lambda x: {'text': x['text']})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    return tokenizer(ex['text'], truncation=True, max_length=128)\n",
    "\n",
    "tok_ds = ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)\n",
    "tok_ds.set_format(type='torch')\n",
    "\n",
    "training_args = TrainingArguments(output_dir='./fine_tuned_jokes',\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  num_train_epochs=3,\n",
    "                                  save_steps=500)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=tok_ds)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "## Try it (exécution)\n",
    "- Exécutez les cellules dans l'ordre: installation -> génération -> filtrage.\n",
    "- Si vous avez des modèles locaux pour la génération, modifiez `model_name_or_path` pour pointer vers le répertoire du modèle.\n",
    "- Vérifiez que `datasets/shortjokes.csv` contient une colonne `text` ou adaptez le nom de colonne dans le squelette.\n",
    "\n",
    "---\n",
    "Si vous voulez, je peux: fournir un script standalone, préparer un fine-tuning complet avec `accelerate`, ou générer un petit jeu d'évaluation pour mesurer la qualité des blagues filtrées."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
