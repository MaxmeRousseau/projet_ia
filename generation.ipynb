{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d81430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération + filtrage (mode CPU pour débogage)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "device = torch.device('cpu')  # START with CPU to avoid CUDA errors; change to 'cuda' si stable\n",
    "\n",
    "# -------- génération (remplacez si vous avez un LM local) ----------\n",
    "gen_name = 'openai-community/gpt2-large'  # remplacez par votre modèle génératif FR si vous en avez\n",
    "tok_gen = AutoTokenizer.from_pretrained(gen_name)\n",
    "if tok_gen.pad_token is None:\n",
    "    tok_gen.pad_token = tok_gen.eos_token\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(gen_name).to(device)\n",
    "\n",
    "def generate_jokes(prompt, n=8, max_len=80, temp=0.9, top_p=0.95):\n",
    "    inputs = tok_gen(prompt, return_tensors='pt')\n",
    "    # don't move tokenizer tensors to device before fixing ids\n",
    "    out = model_gen.generate(**{k: v.to(device) for k, v in inputs.items()}, do_sample=True, temperature=temp, top_p=top_p, max_length=max_len, num_return_sequences=n, pad_token_id=tok_gen.eos_token_id)\n",
    "    results = [tok_gen.decode(o, skip_special_tokens=True)[len(prompt):].strip() for o in out]\n",
    "    return results\n",
    "\n",
    "# -------- charger vos classifieurs (ils n'ont que config+poids, on utilise fallback tokenizer) ----------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def choose_tokenizer_candidate_for_vocab(vocab_size, cfg_archs=None):\n",
    "    # heuristiques simples\n",
    "    if vocab_size is None:\n",
    "        return ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased']\n",
    "    if vocab_size == 30522:\n",
    "        return ['bert-base-uncased', 'distilbert-base-uncased', 'bert-base-cased']\n",
    "    if vocab_size >= 100000:\n",
    "        return ['bert-base-multilingual-cased', 'distilbert-base-multilingual-cased']\n",
    "    # fallback list\n",
    "    return ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased', 'bert-base-uncased']\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer_classif(path):\n",
    "    path = os.path.abspath(path)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Le chemin {path} n'existe pas\")\n",
    "        return None, None\n",
    "    # load model first\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement modèle depuis {path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # tokenizers: try local, else choose candidate by vocab_size\n",
    "    tok = None\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(path)\n",
    "        print(f'Tokenizer local chargé depuis {path}')\n",
    "    except Exception as e_tok:\n",
    "        cfg = {}\n",
    "        try:\n",
    "            with open(os.path.join(path, 'config.json'), 'r', encoding='utf-8') as f:\n",
    "                cfg = json.load(f)\n",
    "        except Exception:\n",
    "            cfg = {}\n",
    "        vocab_size = getattr(model.config, 'vocab_size', None)\n",
    "        candidates = choose_tokenizer_candidate_for_vocab(vocab_size, cfg.get('architectures', None))\n",
    "        for cand in candidates:\n",
    "            try:\n",
    "                tok = AutoTokenizer.from_pretrained(cand)\n",
    "                print(f'Utilisation du tokenizer de fallback: {cand} pour model vocab_size={vocab_size}')\n",
    "                break\n",
    "            except Exception:\n",
    "                tok = None\n",
    "        if tok is None:\n",
    "            # dernier recours\n",
    "            tok = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "            print('Utilisation du tokenizer fallback final: bert-base-multilingual-cased')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tok\n",
    "\n",
    "m1, t1 = load_model_and_tokenizer_classif('./humor_detection_model01')\n",
    "m2, t2 = load_model_and_tokenizer_classif('./humor_model_multilingual')\n",
    "\n",
    "# -------- scoring manuel (sans pipeline) ----------\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def safe_fix_input_ids_for_model(input_ids, model_vocab_size, unk_id=0):\n",
    "    # input_ids: torch.LongTensor\n",
    "    if model_vocab_size is None:\n",
    "        return input_ids\n",
    "    # replace indices >= model_vocab_size with unk_id\n",
    "    mask = input_ids >= model_vocab_size\n",
    "    if mask.any():\n",
    "        input_ids = input_ids.clone()\n",
    "        input_ids[mask] = unk_id\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def score_with_model(texts, model, tokenizer, max_length=128):\n",
    "    scores = []\n",
    "    if model is None or tokenizer is None:\n",
    "        return [0.0] * len(texts)\n",
    "    model_vocab = getattr(model.config, 'vocab_size', None)\n",
    "    unk_id = tokenizer.unk_token_id if tokenizer.unk_token_id is not None else tokenizer.pad_token_id or 0\n",
    "    for t in texts:\n",
    "        try:\n",
    "            enc = tokenizer(t, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt', return_token_type_ids=False)\n",
    "            # fix any out-of-range token ids before moving to device\n",
    "            enc_input_ids = safe_fix_input_ids_for_model(enc['input_ids'], model_vocab, unk_id)\n",
    "            enc['input_ids'] = enc_input_ids\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                logits = model(**enc).logits\n",
    "                probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "            id2label = getattr(model.config, 'id2label', None)\n",
    "            if id2label:\n",
    "                labels = [id2label[i] for i in range(len(probs))]\n",
    "            else:\n",
    "                labels = [f'LABEL_{i}' for i in range(len(probs))]\n",
    "            probs_dict = {labels[i]: float(probs[i]) for i in range(len(probs))}\n",
    "            h = None\n",
    "            for L, p in probs_dict.items():\n",
    "                if 'humor' in L.lower() or 'humour' in L.lower():\n",
    "                    h = p\n",
    "                    break\n",
    "            if h is None:\n",
    "                h = probs_dict.get('LABEL_1', max(probs_dict.values()))\n",
    "        except Exception as e:\n",
    "            print('Erreur scoring:', e)\n",
    "            h = 0.0\n",
    "        scores.append(float(h))\n",
    "    return scores\n",
    "\n",
    "# -------- usage ----------\n",
    "prompt = \"Génère une blague courte en français :\\n\"\n",
    "candidates = generate_jokes(prompt, n=8)\n",
    "scores1 = score_with_model(candidates, m1, t1)\n",
    "scores2 = score_with_model(candidates, m2, t2)\n",
    "combined = [(a + b) / 2.0 for a, b in zip(scores1, scores2)]\n",
    "order = np.argsort(combined)[::-1]\n",
    "for i in order:\n",
    "    print(\"SCORE\", combined[i])\n",
    "    print(candidates[i])\n",
    "    print(\"-----\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
