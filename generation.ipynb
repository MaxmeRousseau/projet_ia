{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d81430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération + filtrage (mode CPU pour débogage)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cpu')  # START with CPU to avoid CUDA errors; change to 'cuda' si stable\n",
    "\n",
    "# -------- génération (remplacez si vous avez un LM local) ----------\n",
    "gen_name = 'gpt2'  # remplacez par votre modèle génératif FR si vous en avez\n",
    "tok_gen = AutoTokenizer.from_pretrained(gen_name)\n",
    "if tok_gen.pad_token is None:\n",
    "    tok_gen.pad_token = tok_gen.eos_token\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(gen_name).to(device)\n",
    "\n",
    "def generate_jokes(prompt, n=8, max_len=80, temp=0.9, top_p=0.95):\n",
    "    inputs = tok_gen(prompt, return_tensors='pt').to(device)\n",
    "    out = model_gen.generate(**inputs, do_sample=True, temperature=temp, top_p=top_p, max_length=max_len, num_return_sequences=n, pad_token_id=tok_gen.eos_token_id)\n",
    "    results = [tok_gen.decode(o, skip_special_tokens=True)[len(prompt):].strip() for o in out]\n",
    "    return results\n",
    "\n",
    "# -------- charger vos classifieurs (ils n'ont que config+poids, on utilise fallback tokenizer) ----------\n",
    "from transformers import AutoTokenizer\n",
    "import os, json\n",
    "\n",
    "def load_model_and_tokenizer_classif(path):\n",
    "    path = os.path.abspath(path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path).to(device)\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(path)\n",
    "    except Exception:\n",
    "        # fallback heuristique\n",
    "        tok = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "    return model, tok\n",
    "\n",
    "m1, t1 = load_model_and_tokenizer_classif('./humor_detection_model01')\n",
    "m2, t2 = load_model_and_tokenizer_classif('./humor_model_multilingual')\n",
    "\n",
    "# -------- scoring manuel (sans pipeline) ----------\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def score_with_model(texts, model, tokenizer):\n",
    "    scores = []\n",
    "    model.eval()\n",
    "    for t in texts:\n",
    "        enc = tokenizer(t, truncation=True, padding=True, return_tensors='pt', return_token_type_ids=False)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        # heuristic: if id2label exists, try to map; else use LABEL_1 or max\n",
    "        id2label = getattr(model.config, 'id2label', None)\n",
    "        if id2label:\n",
    "            labels = [id2label[i] for i in range(len(probs))]\n",
    "        else:\n",
    "            labels = [f'LABEL_{i}' for i in range(len(probs))]\n",
    "        probs_dict = {labels[i]: float(probs[i]) for i in range(len(probs))}\n",
    "        h = None\n",
    "        for L, p in probs_dict.items():\n",
    "            if 'humor' in L.lower() or 'humour' in L.lower():\n",
    "                h = p\n",
    "                break\n",
    "        if h is None:\n",
    "            h = probs_dict.get('LABEL_1', max(probs_dict.values()))\n",
    "        scores.append(float(h))\n",
    "    return scores\n",
    "\n",
    "# -------- usage ----------\n",
    "prompt = \"Génère une blague courte en français :\\n\"\n",
    "candidates = generate_jokes(prompt, n=8)\n",
    "scores1 = score_with_model(candidates, m1, t1)\n",
    "scores2 = score_with_model(candidates, m2, t2)\n",
    "combined = [(a + b) / 2.0 for a, b in zip(scores1, scores2)]\n",
    "order = np.argsort(combined)[::-1]\n",
    "for i in order:\n",
    "    print(\"SCORE\", combined[i])\n",
    "    print(candidates[i])\n",
    "    print(\"-----\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
