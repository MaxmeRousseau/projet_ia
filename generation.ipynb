{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24f380",
   "metadata": {},
   "source": [
    "# Génération de blagues en français\n",
    "Ce notebook montre plusieurs approches pour générer des blagues en français en partant de modèles que vous avez déjà entraînés pour détecter l'humour.\n",
    "\n",
    "Approches recommandées :\n",
    "- Prompt engineering : utiliser un modèle causal pré-entraîné (idéalement en français ou multilingue) et concevoir des prompts qui guident la génération.\n",
    "- Filtrage / reranking : générer plusieurs candidats puis utiliser vos modèles de détection (`humor_detection_model01`, `humor_model_multilingual`) pour scorer et garder les meilleures blagues.\n",
    "- Fine-tuning : si vous avez un dataset de blagues (`datasets/shortjokes.csv`), fine-tuner un modèle causal sur ce corpus donne généralement de meilleurs résultats stylistiques et linguistiques.\n",
    "\n",
    "Dans les cellules suivantes :\n",
    "1) Exemple de génération avec `transformers` (pipeline text-generation)\n",
    "2) Exemple de filtrage / reranking en chargeant vos classifieurs locaux\n",
    "3) Squelette pour fine-tuning si vous souhaitez entraîner un modèle génératif sur vos blagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84921af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: génération de blagues en français\n",
    "# Pré-requis: transformers, torch, datasets installés (voir la cellule d'en haut)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Choix du modèle: ici on utilise un modèle causal multilingue léger si disponible.\n",
    "# Remplacez 'gpt2' par le chemin vers un modèle local (ex: './humor_model_multilingual') ou HF id.\n",
    "model_name_or_path = 'gpt2'  # remplacer par un modèle FR ou local si vous en avez\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "# GPT-2 n'a pas de token pad par défaut, définir pad_token si absent\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "if device == 0:\n",
    "    model = model.to('cuda')\n",
    "\n",
    "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Exemple de fonction de génération simple en français\n",
    "def generate_jokes(prompt, max_length=60, temperature=1.0, top_p=0.9, num_return_sequences=5):\n",
    "    full_prompt = prompt if prompt.endswith('\\n') else prompt + '\\n'\n",
    "    outputs = gen(full_prompt, max_length=max_length, do_sample=True, temperature=temperature, top_p=top_p, num_return_sequences=num_return_sequences)\n",
    "    return [o['generated_text'][len(full_prompt):].strip() for o in outputs]\n",
    "\n",
    "# Exemples d'appels\n",
    "prompt = 'Génère une blague courte en français :'\n",
    "jokes = generate_jokes(prompt, max_length=80, temperature=0.9, top_p=0.95, num_return_sequences=8)\n",
    "for i,j in enumerate(jokes,1):\n",
    "    print(f'--- Blague {i} ---')\n",
    "    print(j)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage / reranking avec vos modèles de détection d'humour locaux (implémentation robuste)\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Chemins vers vos modèles locaux (existant dans le repo)\n",
    "local_model_1 = './humor_detection_model01'\n",
    "local_model_2 = './humor_model_multilingual'\n",
    "\n",
    "def load_model_and_tokenizer(path):\n",
    "    try:\n",
    "        abs_path = os.path.abspath(path) if path is not None else None\n",
    "        if abs_path is None or not os.path.exists(abs_path):\n",
    "            print(f\"Le chemin {path} n'existe pas (résolu en {abs_path}).\")\n",
    "            return None, None\n",
    "        try:\n",
    "            print(f'Chargement du modèle depuis: {abs_path} -> contenu:', os.listdir(abs_path))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Charger le modèle (doit fonctionner si config.json + poids présents)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(abs_path)\n",
    "\n",
    "        # Déterminer un tokenizer adapté : essayer d'abord le tokenizer local, sinon choisir selon model_type dans config\n",
    "        tok = None\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(abs_path)\n",
    "        except Exception as e_tok:\n",
    "            print(f\"Pas de tokenizer local trouvé dans {abs_path} (erreur: {e_tok}). Recherche d'un tokenizer adapté selon config...\")\n",
    "            # lire config pour détecter model_type\n",
    "            cfg_path = os.path.join(abs_path, 'config.json')\n",
    "            try:\n",
    "                with open(cfg_path, 'r', encoding='utf-8') as f:\n",
    "                    cfg = json.load(f)\n",
    "                    model_type = cfg.get('model_type', '')\n",
    "            except Exception:\n",
    "                model_type = ''\n",
    "            # heuristiques de fallback\n",
    "            tried = []\n",
    "            if 'distilbert' in model_type.lower() or any('Distil' in a for a in cfg.get('architectures', [])):\n",
    "                tried.append('distilbert-base-multilingual-cased')\n",
    "            tried.extend(['bert-base-multilingual-cased', 'distilbert-base-uncased', 'bert-base-uncased'])\n",
    "            for candidate in tried:\n",
    "                try:\n",
    "                    tok = AutoTokenizer.from_pretrained(candidate)\n",
    "                    print(f'Utilisation du tokenizer de fallback: {candidate}')\n",
    "                    break\n",
    "                except Exception:\n",
    "                    tok = None\n",
    "            if tok is None:\n",
    "                # dernier recours\n",
    "                tok = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "                print('Utilisation du tokenizer fallback final: bert-base-multilingual-cased')\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        return model, tok\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement {path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "model1, tok1 = load_model_and_tokenizer(local_model_1)\n",
    "model2, tok2 = load_model_and_tokenizer(local_model_2)\n",
    "\n",
    "def score_jokes_with_model(jokes, model, tokenizer, device=None, max_length=128):\n",
    "    if model is None or tokenizer is None:\n",
    "        return [0.0] * len(jokes)\n",
    "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    scores = []\n",
    "    # id2label si présent\n",
    "    id2label = getattr(model.config, 'id2label', None)\n",
    "    for j in jokes:\n",
    "        try:\n",
    "            inputs = tokenizer(j, truncation=True, padding=True, max_length=max_length, return_tensors='pt', return_token_type_ids=False)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "            # construire mapping label->prob\n",
    "            if id2label is not None:\n",
    "                labels = [id2label[i] for i in range(len(probs))]\n",
    "            else:\n",
    "                labels = [f'LABEL_{i}' for i in range(len(probs))]\n",
    "            probs_dict = {labels[i]: float(probs[i]) for i in range(len(probs))}\n",
    "            # heuristique pour probabilité d'humour\n",
    "            h = None\n",
    "            for key in probs_dict:\n",
    "                if 'humor' in key.lower() or 'humour' in key.lower():\n",
    "                    h = probs_dict[key]\n",
    "                    break\n",
    "            if h is None:\n",
    "                h = probs_dict.get('LABEL_1', max(probs_dict.values()))\n",
    "        except Exception as e:\n",
    "            print('Erreur scoring:', e)\n",
    "            h = 0.0\n",
    "        scores.append(float(h))\n",
    "    return scores\n",
    "\n",
    "# Exécuter le scoring et reranking si la variable `jokes` existe\n",
    "try:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    scores1 = score_jokes_with_model(jokes, model1, tok1, device)\n",
    "    scores2 = score_jokes_with_model(jokes, model2, tok2, device)\n",
    "    combined = [(s1 + s2) / 2.0 for s1, s2 in zip(scores1, scores2)]\n",
    "\n",
    "    order = np.argsort(combined)[::-1]\n",
    "    print('Reranking des blagues par score moyen de humour :')\n",
    "    for idx in order:\n",
    "        print('--- Blague (score=', round(combined[idx], 3), ') ---')\n",
    "        print(jokes[idx])\n",
    "        print()\n",
    "except NameError:\n",
    "    print(\"La variable 'jokes' n'existe pas : exécutez d'abord la cellule de génération pour créer des candidats de blagues.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935dc781",
   "metadata": {},
   "source": [
    "## Fine-tuning minimal (squelette)\n",
    "Si vous souhaitez fine-tuner un modèle causal sur `datasets/shortjokes.csv` :\n",
    "1. Charger le CSV et construire un dataset texte (une blague par exemple).\n",
    "2. Tokenizer et préparer les exemples pour le modèle causal (concatenate prompt+target si besoin).\n",
    "3. Utiliser `transformers.Trainer` ou `accelerate` pour l'entraînement.\n",
    "\n",
    "Exemple minimal :\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "ds = load_dataset('csv', data_files='datasets/shortjokes.csv')['train']\n",
    "# Supposons que la colonne s'appelle 'text'\n",
    "ds = ds.map(lambda x: {'text': x['text']})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    return tokenizer(ex['text'], truncation=True, max_length=128)\n",
    "\n",
    "tok_ds = ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)\n",
    "tok_ds.set_format(type='torch')\n",
    "\n",
    "training_args = TrainingArguments(output_dir='./fine_tuned_jokes',\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  num_train_epochs=3,\n",
    "                                  save_steps=500)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=tok_ds)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "## Try it (exécution)\n",
    "- Exécutez les cellules dans l'ordre: installation -> génération -> filtrage.\n",
    "- Si vous avez des modèles locaux pour la génération, modifiez `model_name_or_path` pour pointer vers le répertoire du modèle.\n",
    "- Vérifiez que `datasets/shortjokes.csv` contient une colonne `text` ou adaptez le nom de colonne dans le squelette.\n",
    "\n",
    "---\n",
    "Si vous voulez, je peux: fournir un script standalone, préparer un fine-tuning complet avec `accelerate`, ou générer un petit jeu d'évaluation pour mesurer la qualité des blagues filtrées."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
