{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/processed/colbert_humor.csv')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(dataset)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"openai-community/gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Some pretrained tokenizers (GPT-2 style) don't have a pad_token. Trainer/dataset preprocessing\n",
    "# may request padding when using `padding='max_length'` or batched tokenization.\n",
    "# Ensure a pad_token exists to avoid ValueError from the tokenizer padding/truncation checks.\n",
    "if tokenizer.pad_token is None:\n",
    "    # Use eos_token as pad token (safe choice for causal LM tokenizers)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# Make sure model config knows the pad_token_id. Some model classes don't set it automatically.\n",
    "if getattr(model.config, 'pad_token_id', None) is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "# Remove only columns that are actually present (avoid ValueError). Do not remove 'text' before tokenization.\n",
    "cols_to_try = ['__index_level_0__']\n",
    "cols_present = [c for c in cols_to_try if c in encoded_dataset['train'].column_names]\n",
    "if cols_present:\n",
    "    encoded_dataset = encoded_dataset.remove_columns(cols_present)\n",
    "\n",
    "# Ensure the label column is named 'labels' because Trainer expects 'labels' by default\n",
    "if 'labels' not in encoded_dataset['train'].column_names:\n",
    "    label_candidate = 'humor' if 'humor' in encoded_dataset['train'].column_names else ('label' if 'label' in encoded_dataset['train'].column_names else None)\n",
    "    if label_candidate is None:\n",
    "        raise ValueError(f\"No label column found. Available columns: {encoded_dataset['train'].column_names}\")\n",
    "    # When using batched=True the function receives lists; convert labels to int to force CrossEntropyLoss (not BCEWithLogits)\n",
    "    def _add_labels(batch):\n",
    "        labels = batch[label_candidate]\n",
    "        batch['labels'] = [int(l) for l in labels]\n",
    "        return batch\n",
    "    encoded_dataset = encoded_dataset.map(_add_labels, batched=True)\n",
    "    # Optionally remove the old label column\n",
    "    if label_candidate in encoded_dataset['train'].column_names:\n",
    "        encoded_dataset = encoded_dataset.remove_columns([label_candidate])\n",
    "\n",
    "# Ensure labels have integer dtype in the dataset features (best-effort)\n",
    "try:\n",
    "    from datasets import Value\n",
    "    encoded_dataset = encoded_dataset.cast_column('labels', Value('int64'))\n",
    "except Exception:\n",
    "    # cast_column may fail depending on datasets version; we've already converted to int above\n",
    "    pass\n",
    "\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./finetuned_gpt2large',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='binary')\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./finetuned_gpt2large')\n",
    "tokenizer.save_pretrained('./finetuned_gpt2large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d81430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération + filtrage (mode CPU pour débogage)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "device = torch.device('cpu')  # START with CPU to avoid CUDA errors; change to 'cuda' si stable\n",
    "\n",
    "# -------- génération (remplacez si vous avez un LM local) ----------\n",
    "gen_name = 'openai-community/gpt2-large'  # remplacez par votre modèle génératif FR si vous en avez\n",
    "tok_gen = AutoTokenizer.from_pretrained(gen_name)\n",
    "if tok_gen.pad_token is None:\n",
    "    tok_gen.pad_token = tok_gen.eos_token\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(gen_name).to(device)\n",
    "\n",
    "def generate_jokes(prompt, n=8, max_len=80, temp=0.9, top_p=0.95):\n",
    "    inputs = tok_gen(prompt, return_tensors='pt')\n",
    "    # don't move tokenizer tensors to device before fixing ids\n",
    "    out = model_gen.generate(**{k: v.to(device) for k, v in inputs.items()}, do_sample=True, temperature=temp, top_p=top_p, max_length=max_len, num_return_sequences=n, pad_token_id=tok_gen.eos_token_id)\n",
    "    results = [tok_gen.decode(o, skip_special_tokens=True)[len(prompt):].strip() for o in out]\n",
    "    return results\n",
    "\n",
    "# -------- charger vos classifieurs (ils n'ont que config+poids, on utilise fallback tokenizer) ----------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def choose_tokenizer_candidate_for_vocab(vocab_size, cfg_archs=None):\n",
    "    # heuristiques simples\n",
    "    if vocab_size is None:\n",
    "        return ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased']\n",
    "    if vocab_size == 30522:\n",
    "        return ['bert-base-uncased', 'distilbert-base-uncased', 'bert-base-cased']\n",
    "    if vocab_size >= 100000:\n",
    "        return ['bert-base-multilingual-cased', 'distilbert-base-multilingual-cased']\n",
    "    # fallback list\n",
    "    return ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased', 'bert-base-uncased']\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer_classif(path):\n",
    "    path = os.path.abspath(path)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Le chemin {path} n'existe pas\")\n",
    "        return None, None\n",
    "    # load model first\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement modèle depuis {path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # tokenizers: try local, else choose candidate by vocab_size\n",
    "    tok = None\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(path)\n",
    "        print(f'Tokenizer local chargé depuis {path}')\n",
    "    except Exception as e_tok:\n",
    "        cfg = {}\n",
    "        try:\n",
    "            with open(os.path.join(path, 'config.json'), 'r', encoding='utf-8') as f:\n",
    "                cfg = json.load(f)\n",
    "        except Exception:\n",
    "            cfg = {}\n",
    "        vocab_size = getattr(model.config, 'vocab_size', None)\n",
    "        candidates = choose_tokenizer_candidate_for_vocab(vocab_size, cfg.get('architectures', None))\n",
    "        for cand in candidates:\n",
    "            try:\n",
    "                tok = AutoTokenizer.from_pretrained(cand)\n",
    "                print(f'Utilisation du tokenizer de fallback: {cand} pour model vocab_size={vocab_size}')\n",
    "                break\n",
    "            except Exception:\n",
    "                tok = None\n",
    "        if tok is None:\n",
    "            # dernier recours\n",
    "            tok = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "            print('Utilisation du tokenizer fallback final: bert-base-multilingual-cased')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tok\n",
    "\n",
    "m1, t1 = load_model_and_tokenizer_classif('./humor_detection_model01')\n",
    "m2, t2 = load_model_and_tokenizer_classif('./humor_model_multilingual')\n",
    "\n",
    "# -------- scoring manuel (sans pipeline) ----------\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def safe_fix_input_ids_for_model(input_ids, model_vocab_size, unk_id=0):\n",
    "    # input_ids: torch.LongTensor\n",
    "    if model_vocab_size is None:\n",
    "        return input_ids\n",
    "    # replace indices >= model_vocab_size with unk_id\n",
    "    mask = input_ids >= model_vocab_size\n",
    "    if mask.any():\n",
    "        input_ids = input_ids.clone()\n",
    "        input_ids[mask] = unk_id\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def score_with_model(texts, model, tokenizer, max_length=128):\n",
    "    scores = []\n",
    "    if model is None or tokenizer is None:\n",
    "        return [0.0] * len(texts)\n",
    "    model_vocab = getattr(model.config, 'vocab_size', None)\n",
    "    unk_id = tokenizer.unk_token_id if tokenizer.unk_token_id is not None else tokenizer.pad_token_id or 0\n",
    "    for t in texts:\n",
    "        try:\n",
    "            enc = tokenizer(t, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt', return_token_type_ids=False)\n",
    "            # fix any out-of-range token ids before moving to device\n",
    "            enc_input_ids = safe_fix_input_ids_for_model(enc['input_ids'], model_vocab, unk_id)\n",
    "            enc['input_ids'] = enc_input_ids\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                logits = model(**enc).logits\n",
    "                probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "            id2label = getattr(model.config, 'id2label', None)\n",
    "            if id2label:\n",
    "                labels = [id2label[i] for i in range(len(probs))]\n",
    "            else:\n",
    "                labels = [f'LABEL_{i}' for i in range(len(probs))]\n",
    "            probs_dict = {labels[i]: float(probs[i]) for i in range(len(probs))}\n",
    "            h = None\n",
    "            for L, p in probs_dict.items():\n",
    "                if 'humor' in L.lower() or 'humour' in L.lower():\n",
    "                    h = p\n",
    "                    break\n",
    "            if h is None:\n",
    "                h = probs_dict.get('LABEL_1', max(probs_dict.values()))\n",
    "        except Exception as e:\n",
    "            print('Erreur scoring:', e)\n",
    "            h = 0.0\n",
    "        scores.append(float(h))\n",
    "    return scores\n",
    "\n",
    "# -------- usage ----------\n",
    "prompt = \"Génère une blague courte en français :\\n\"\n",
    "candidates = generate_jokes(prompt, n=8)\n",
    "scores1 = score_with_model(candidates, m1, t1)\n",
    "scores2 = score_with_model(candidates, m2, t2)\n",
    "combined = [(a + b) / 2.0 for a, b in zip(scores1, scores2)]\n",
    "order = np.argsort(combined)[::-1]\n",
    "for i in order:\n",
    "    print(\"SCORE\", combined[i])\n",
    "    print(candidates[i])\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder un tokenizer de fallback dans les dossiers de classifieurs (rend le chargement déterministe)\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "def ensure_tokenizer_saved(folder, tokenizer_name='distilbert-base-multilingual-cased'):\n",
    "    folder = os.path.abspath(folder)\n",
    "    if not os.path.exists(folder):\n",
    "        print(f'Folder {folder} nexiste pas, skip')\n",
    "        return\n",
    "    # check if there's already a tokenizer file\n",
    "    files = os.listdir(folder)\n",
    "    if any(f.startswith('tokenizer') or f in ('vocab.txt','tokenizer.json','tokenizer_config.json') for f in files):\n",
    "        print(f'Tokenizer déjà présent dans {folder}, skip')\n",
    "        return\n",
    "    print(f'Téléchargement et sauvegarde du tokenizer {tokenizer_name} dans {folder}')\n",
    "    tok = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tok.save_pretrained(folder)\n",
    "\n",
    "ensure_tokenizer_saved('./humor_detection_model01')\n",
    "ensure_tokenizer_saved('./humor_model_multilingual')\n",
    "print('Tokenizers saved (ou déjà présents).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeadad0",
   "metadata": {},
   "source": [
    "## Diagnostic GPU / CUDA\n",
    "Si vous obtenez encore `AcceleratorError: CUDA error: device-side assert triggered`, exécutez la cellule de diagnostic suivante pour collecter l'état de CUDA et PyTorch.\n",
    "Cela fournit des informations utiles (CUDA_VISIBLE_DEVICES, torch.cuda.is_available(), nombre de GPUs, versions) et les commandes recommandées à lancer depuis un terminal pour un debug plus profond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030437c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule de diagnostic pour CUDA / PyTorch\n",
    "import os, sys, subprocess, json\n",
    "def run_cmd(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        out = e.output\n",
    "    return out\n",
    "\n",
    "print('Environment variables relevant to CUDA:')\n",
    "for v in ('CUDA_VISIBLE_DEVICES','CUDA_LAUNCH_BLOCKING','TORCH_USE_CUDA_DSA'):\n",
    "    print(v, '=', os.environ.get(v))\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print('torch version:', torch.__version__)\n",
    "    print('cuda available:', torch.cuda.is_available())\n",
    "    try:\n",
    "        print('cuda device count:', torch.cuda.device_count())\n",
    "        print('current device:', torch.cuda.current_device())\n",
    "        print('device name:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    except Exception as e:\n",
    "        print('cuda info error:', e)\n",
    "except Exception as e:\n",
    "    print('Could not import torch:', e)\n",
    "\n",
    "# Recommended terminal commands to run if CUDA errors persist\n",
    "print('Recommended terminal commands (run in a shell):')\n",
    "print('1) Restart Jupyter / kernel to clear CUDA state')\n",
    "print('2) Run with synchronous CUDA errors to get a proper backtrace:')\n",
    "print('   CUDA_LAUNCH_BLOCKING=1 jupyter lab')\n",
    "print('   or to test availability: CUDA_LAUNCH_BLOCKING=1 python -c ')\n",
    "print('3) Optionally enable device-side assertions for debug:')\n",
    "print('   export TORCH_USE_CUDA_DSA=1')\n",
    "print('4) If the crash happens during Trainer creation, try running the fine-tuning cell with force_cpu=True and restart the kernel')\n",
    "\n",
    "# Quick check: nvidia-smi if present\n",
    "out = run_cmd('which nvidia-smi >/dev/null 2>&1 && nvidia-smi -L || echo ')\n",
    "print('' + out)\n",
    "\n",
    "print('Done. If you want, paste the above output here and I will suggest next steps.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
