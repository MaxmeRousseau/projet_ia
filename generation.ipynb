{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning minimal de GPT2-Large sur le dataset de blagues (colbert_humor.csv)\n",
    "# Pré-requis: avoir installé `transformers`, `datasets` et (optionnel) `accelerate`.\n",
    "# IMPORTANT: GPT2-large est gourmand en GPU/mémoire. Préférer lancer sur GPU/accelerate.\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "# Chemins et hyperparamètres (ajustez selon votre GPU)\n",
    "csv_path = 'data/processed/colbert_humor.csv'\n",
    "model_name = 'openai-community/gpt2-large'  # ou 'gpt2-large' selon disponibilité\n",
    "output_dir = './fine_tuned_gpt2_large'\n",
    "block_size = 128\n",
    "num_epochs = 3\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Charger dataset (on suppose une colonne 'text' ou 'text_clean')\n",
    "raw = load_dataset('csv', data_files=csv_path)['train']\n",
    "print('Taille dataset:', len(raw))\n",
    "# Normaliser la colonne texte\n",
    "def pick_text(x):\n",
    "    for c in ('text_clean', 'text'):\n",
    "        if c in x and x[c] is not None:\n",
    "            return x[c]\n",
    "    return ''\n",
    "raw = raw.map(lambda x: {'text': pick_text(x)})\n",
    "raw = raw.filter(lambda x: x['text'] is not None and x['text'].strip() != '')\n",
    "\n",
    "# Forcer CPU pour éviter les erreurs CUDA lors de l'initialisation (décommenter pour utiliser GPU)\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''  # force CPU-only (keeps trainer safe for debugging)\n",
    "print('Chargement du tokenizer et modèle (cela peut télécharger des centaines de MB)')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# GPT2 n'a pas de token pad par défaut\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization (line-by-line)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized = raw.map(tokenize_function, batched=True, remove_columns=raw.column_names)\n",
    "\n",
    "# Grouper en blocks pour la LM (concat puis split)\n",
    "def group_texts(examples):\n",
    "    # examples['input_ids'] est une liste de listes\n",
    "    input_ids = examples['input_ids']\n",
    "    # concatenate all input ids for the batch\n",
    "    concatenated = []\n",
    "    for ids in input_ids:\n",
    "        concatenated.extend(ids)\n",
    "    total_length = (len(concatenated) // block_size) * block_size\n",
    "    if total_length == 0:\n",
    "        return {\"input_ids\": [], \"labels\": []}\n",
    "    result_input_ids = [concatenated[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "    return {'input_ids': result_input_ids, 'labels': [r.copy() for r in result_input_ids]}\n",
    "\n",
    "# IMPORTANT: retirer les colonnes non utilisées pour éviter des incohérences de longueur lors de l'écriture Arrow\n",
    "# réduire batch_size si dataset petit / problème mémoire\n",
    "lm_datasets = tokenized.map(group_texts, batched=True, batch_size=200, remove_columns=tokenized.column_names)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "# Instantiate Trainer safely: if CUDA device-side asserts happen during seed setting,\n",
    "# we fallback to a CPU-only Trainer. If you want GPU training later, remove the\n",
    "# os.environ override above and run with accelerate or ensure CUDA is stable.\n",
    "trainer = None\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print('Erreur lors de la création du Trainer (probablement liée à CUDA).')\n",
    "    print('Exception:', e)\n",
    "    print('Réessai sur CPU: désactivation des GPUs via CUDA_VISIBLE_DEVICES=puis recréation du Trainer.')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    import importlib, sys\n",
    "    # Re-import torch after env change can be necessary in some contexts; here we just retry Trainer creation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "print(\"Démarrage de l'entraînement. Sur CPU ce sera très lent — préférez GPU/accelerate.\")\n",
    "# Lancez trainer.train() manuellement (décommenter la ligne ci‑dessous pour exécuter ici)\n",
    "# trainer.train()\n",
    "\n",
    "print(\"Cellule de fine-tuning prête. Décommentez trainer.train() pour lancer l'entraînement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d81430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération + filtrage (mode CPU pour débogage)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "device = torch.device('cpu')  # START with CPU to avoid CUDA errors; change to 'cuda' si stable\n",
    "\n",
    "# -------- génération (remplacez si vous avez un LM local) ----------\n",
    "gen_name = 'openai-community/gpt2-large'  # remplacez par votre modèle génératif FR si vous en avez\n",
    "tok_gen = AutoTokenizer.from_pretrained(gen_name)\n",
    "if tok_gen.pad_token is None:\n",
    "    tok_gen.pad_token = tok_gen.eos_token\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(gen_name).to(device)\n",
    "\n",
    "def generate_jokes(prompt, n=8, max_len=80, temp=0.9, top_p=0.95):\n",
    "    inputs = tok_gen(prompt, return_tensors='pt')\n",
    "    # don't move tokenizer tensors to device before fixing ids\n",
    "    out = model_gen.generate(**{k: v.to(device) for k, v in inputs.items()}, do_sample=True, temperature=temp, top_p=top_p, max_length=max_len, num_return_sequences=n, pad_token_id=tok_gen.eos_token_id)\n",
    "    results = [tok_gen.decode(o, skip_special_tokens=True)[len(prompt):].strip() for o in out]\n",
    "    return results\n",
    "\n",
    "# -------- charger vos classifieurs (ils n'ont que config+poids, on utilise fallback tokenizer) ----------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def choose_tokenizer_candidate_for_vocab(vocab_size, cfg_archs=None):\n",
    "    # heuristiques simples\n",
    "    if vocab_size is None:\n",
    "        return ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased']\n",
    "    if vocab_size == 30522:\n",
    "        return ['bert-base-uncased', 'distilbert-base-uncased', 'bert-base-cased']\n",
    "    if vocab_size >= 100000:\n",
    "        return ['bert-base-multilingual-cased', 'distilbert-base-multilingual-cased']\n",
    "    # fallback list\n",
    "    return ['distilbert-base-multilingual-cased', 'bert-base-multilingual-cased', 'bert-base-uncased']\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer_classif(path):\n",
    "    path = os.path.abspath(path)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Le chemin {path} n'existe pas\")\n",
    "        return None, None\n",
    "    # load model first\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement modèle depuis {path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # tokenizers: try local, else choose candidate by vocab_size\n",
    "    tok = None\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(path)\n",
    "        print(f'Tokenizer local chargé depuis {path}')\n",
    "    except Exception as e_tok:\n",
    "        cfg = {}\n",
    "        try:\n",
    "            with open(os.path.join(path, 'config.json'), 'r', encoding='utf-8') as f:\n",
    "                cfg = json.load(f)\n",
    "        except Exception:\n",
    "            cfg = {}\n",
    "        vocab_size = getattr(model.config, 'vocab_size', None)\n",
    "        candidates = choose_tokenizer_candidate_for_vocab(vocab_size, cfg.get('architectures', None))\n",
    "        for cand in candidates:\n",
    "            try:\n",
    "                tok = AutoTokenizer.from_pretrained(cand)\n",
    "                print(f'Utilisation du tokenizer de fallback: {cand} pour model vocab_size={vocab_size}')\n",
    "                break\n",
    "            except Exception:\n",
    "                tok = None\n",
    "        if tok is None:\n",
    "            # dernier recours\n",
    "            tok = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "            print('Utilisation du tokenizer fallback final: bert-base-multilingual-cased')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tok\n",
    "\n",
    "m1, t1 = load_model_and_tokenizer_classif('./humor_detection_model01')\n",
    "m2, t2 = load_model_and_tokenizer_classif('./humor_model_multilingual')\n",
    "\n",
    "# -------- scoring manuel (sans pipeline) ----------\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def safe_fix_input_ids_for_model(input_ids, model_vocab_size, unk_id=0):\n",
    "    # input_ids: torch.LongTensor\n",
    "    if model_vocab_size is None:\n",
    "        return input_ids\n",
    "    # replace indices >= model_vocab_size with unk_id\n",
    "    mask = input_ids >= model_vocab_size\n",
    "    if mask.any():\n",
    "        input_ids = input_ids.clone()\n",
    "        input_ids[mask] = unk_id\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def score_with_model(texts, model, tokenizer, max_length=128):\n",
    "    scores = []\n",
    "    if model is None or tokenizer is None:\n",
    "        return [0.0] * len(texts)\n",
    "    model_vocab = getattr(model.config, 'vocab_size', None)\n",
    "    unk_id = tokenizer.unk_token_id if tokenizer.unk_token_id is not None else tokenizer.pad_token_id or 0\n",
    "    for t in texts:\n",
    "        try:\n",
    "            enc = tokenizer(t, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt', return_token_type_ids=False)\n",
    "            # fix any out-of-range token ids before moving to device\n",
    "            enc_input_ids = safe_fix_input_ids_for_model(enc['input_ids'], model_vocab, unk_id)\n",
    "            enc['input_ids'] = enc_input_ids\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                logits = model(**enc).logits\n",
    "                probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "            id2label = getattr(model.config, 'id2label', None)\n",
    "            if id2label:\n",
    "                labels = [id2label[i] for i in range(len(probs))]\n",
    "            else:\n",
    "                labels = [f'LABEL_{i}' for i in range(len(probs))]\n",
    "            probs_dict = {labels[i]: float(probs[i]) for i in range(len(probs))}\n",
    "            h = None\n",
    "            for L, p in probs_dict.items():\n",
    "                if 'humor' in L.lower() or 'humour' in L.lower():\n",
    "                    h = p\n",
    "                    break\n",
    "            if h is None:\n",
    "                h = probs_dict.get('LABEL_1', max(probs_dict.values()))\n",
    "        except Exception as e:\n",
    "            print('Erreur scoring:', e)\n",
    "            h = 0.0\n",
    "        scores.append(float(h))\n",
    "    return scores\n",
    "\n",
    "# -------- usage ----------\n",
    "prompt = \"Génère une blague courte en français :\\n\"\n",
    "candidates = generate_jokes(prompt, n=8)\n",
    "scores1 = score_with_model(candidates, m1, t1)\n",
    "scores2 = score_with_model(candidates, m2, t2)\n",
    "combined = [(a + b) / 2.0 for a, b in zip(scores1, scores2)]\n",
    "order = np.argsort(combined)[::-1]\n",
    "for i in order:\n",
    "    print(\"SCORE\", combined[i])\n",
    "    print(candidates[i])\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder un tokenizer de fallback dans les dossiers de classifieurs (rend le chargement déterministe)\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "def ensure_tokenizer_saved(folder, tokenizer_name='distilbert-base-multilingual-cased'):\n",
    "    folder = os.path.abspath(folder)\n",
    "    if not os.path.exists(folder):\n",
    "        print(f'Folder {folder} nexiste pas, skip')\n",
    "        return\n",
    "    # check if there's already a tokenizer file\n",
    "    files = os.listdir(folder)\n",
    "    if any(f.startswith('tokenizer') or f in ('vocab.txt','tokenizer.json','tokenizer_config.json') for f in files):\n",
    "        print(f'Tokenizer déjà présent dans {folder}, skip')\n",
    "        return\n",
    "    print(f'Téléchargement et sauvegarde du tokenizer {tokenizer_name} dans {folder}')\n",
    "    tok = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tok.save_pretrained(folder)\n",
    "\n",
    "ensure_tokenizer_saved('./humor_detection_model01')\n",
    "ensure_tokenizer_saved('./humor_model_multilingual')\n",
    "print('Tokenizers saved (ou déjà présents).')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
