{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets evaluate accelerate timm kagglehub pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24f380",
   "metadata": {},
   "source": [
    "# Génération de blagues en français\n",
    "Ce notebook montre plusieurs approches pour générer des blagues en français en partant de modèles que vous avez déjà entraînés pour détecter l'humour.\n",
    "\n",
    "Approches recommandées :\n",
    "- Prompt engineering : utiliser un modèle causal pré-entraîné (idéalement en français ou multilingue) et concevoir des prompts qui guident la génération.\n",
    "- Filtrage / reranking : générer plusieurs candidats puis utiliser vos modèles de détection (`humor_detection_model01`, `humor_model_multilingual`) pour scorer et garder les meilleures blagues.\n",
    "- Fine-tuning : si vous avez un dataset de blagues (`datasets/shortjokes.csv`), fine-tuner un modèle causal sur ce corpus donne généralement de meilleurs résultats stylistiques et linguistiques.\n",
    "\n",
    "Dans les cellules suivantes :\n",
    "1) Exemple de génération avec `transformers` (pipeline text-generation)\n",
    "2) Exemple de filtrage / reranking en chargeant vos classifieurs locaux\n",
    "3) Squelette pour fine-tuning si vous souhaitez entraîner un modèle génératif sur vos blagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84921af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: génération de blagues en français\n",
    "# Pré-requis: transformers, torch, datasets installés (voir la cellule d'en haut)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Choix du modèle: ici on utilise un modèle causal multilingue léger si disponible.\n",
    "# Remplacez 'gpt2' par le chemin vers un modèle local (ex: './humor_model_multilingual') ou HF id.\n",
    "model_name_or_path = 'gpt2'  # remplacer par un modèle FR ou local si vous en avez\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "# GPT-2 n'a pas de token pad par défaut, définir pad_token si absent\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "if device == 0:\n",
    "    model = model.to('cuda')\n",
    "\n",
    "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Exemple de fonction de génération simple en français\n",
    "def generate_jokes(prompt, max_length=60, temperature=1.0, top_p=0.9, num_return_sequences=5):\n",
    "    full_prompt = prompt if prompt.endswith('\\n') else prompt + '\\n'\n",
    "    outputs = gen(full_prompt, max_length=max_length, do_sample=True, temperature=temperature, top_p=top_p, num_return_sequences=num_return_sequences)\n",
    "    return [o['generated_text'][len(full_prompt):].strip() for o in outputs]\n",
    "\n",
    "# Exemples d'appels\n",
    "prompt = 'Génère une blague courte en français :'\n",
    "jokes = generate_jokes(prompt, max_length=80, temperature=0.9, top_p=0.95, num_return_sequences=8)\n",
    "for i,j in enumerate(jokes,1):\n",
    "    print(f'--- Blague {i} ---')\n",
    "    print(j)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage / reranking avec vos modèles de détection d'humour locaux\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Chemins vers vos modèles locaux (existant dans le repo)\n",
    "local_model_1 = './humor_detection_model01'\n",
    "local_model_2 = './humor_model_multilingual'\n",
    "\n",
    "def load_classifier(path):\n",
    "    try:\n",
    "        abs_path = os.path.abspath(path) if path is not None else None\n",
    "        # Vérifier les fichiers attendus\n",
    "        if abs_path is None or not os.path.exists(abs_path):\n",
    "            print(f\"Le chemin {path} n'existe pas (résolu en {abs_path}).\")\n",
    "            return None\n",
    "        # Listing utile pour debug\n",
    "        try:\n",
    "            print(f'Chargement du modèle depuis: {abs_path} -> contenu:', os.listdir(abs_path))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Tenter de charger un tokenizer local, sinon fallback vers un tokenizer multilingue\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(abs_path)\n",
    "        except Exception as e_tok:\n",
    "            print(f\"Pas de tokenizer local trouvé dans {abs_path} (erreur: {e_tok}). Utilisation d'un tokenizer multilingue en fallback.\")\n",
    "            # Choix de fallback : tokenizer multilingue léger\n",
    "            tok = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "        mdl = AutoModelForSequenceClassification.from_pretrained(abs_path)\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, device=device)\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur chargement {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "clf1 = load_classifier(local_model_1)\n",
    "clf2 = load_classifier(local_model_2)\n",
    "\n",
    "# Fonction de scoring et reranking\n",
    "\n",
    "def score_jokes(jokes, clf):\n",
    "    scores = []\n",
    "    if clf is None:\n",
    "        return [0.0] * len(jokes)\n",
    "    for j in jokes:\n",
    "        try:\n",
    "            res = clf(j)\n",
    "            # res: list (batch) -> list of dicts per class\n",
    "            class_scores = res[0]\n",
    "            probs = {c['label']: c['score'] for c in class_scores}\n",
    "            h = None\n",
    "            for key in probs:\n",
    "                if 'humor' in key.lower() or 'humour' in key.lower():\n",
    "                    h = probs[key]\n",
    "                    break\n",
    "            if h is None:\n",
    "                # si pas de label 'humor', on prend la probabilité de la classe positive si nommée 'LABEL_1' heuristique\n",
    "                # sinon on prend la max\n",
    "                h = probs.get('LABEL_1', max(probs.values()))\n",
    "        except Exception as e:\n",
    "            print('Erreur scoring:', e)\n",
    "            h = 0.0\n",
    "        scores.append(float(h))\n",
    "    return scores\n",
    "\n",
    "# On suppose que la variable `jokes` existe (générée avant)\n",
    "try:\n",
    "    scores1 = score_jokes(jokes, clf1)\n",
    "    scores2 = score_jokes(jokes, clf2)\n",
    "    combined = [(s1 + s2) / 2.0 for s1, s2 in zip(scores1, scores2)]\n",
    "\n",
    "    order = np.argsort(combined)[::-1]\n",
    "    print('Reranking des blagues par score moyen de humour :')\n",
    "    for idx in order:\n",
    "        print('--- Blague (score=', round(combined[idx], 3), ') ---')\n",
    "        print(jokes[idx])\n",
    "        print()\n",
    "except NameError:\n",
    "    print(\"La variable 'jokes' n'existe pas : exécutez d'abord la cellule de génération pour créer des candidats de blagues.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935dc781",
   "metadata": {},
   "source": [
    "## Fine-tuning minimal (squelette)\n",
    "Si vous souhaitez fine-tuner un modèle causal sur `datasets/shortjokes.csv` :\n",
    "1. Charger le CSV et construire un dataset texte (une blague par exemple).\n",
    "2. Tokenizer et préparer les exemples pour le modèle causal (concatenate prompt+target si besoin).\n",
    "3. Utiliser `transformers.Trainer` ou `accelerate` pour l'entraînement.\n",
    "\n",
    "Exemple minimal :\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "ds = load_dataset('csv', data_files='datasets/shortjokes.csv')['train']\n",
    "# Supposons que la colonne s'appelle 'text'\n",
    "ds = ds.map(lambda x: {'text': x['text']})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    return tokenizer(ex['text'], truncation=True, max_length=128)\n",
    "\n",
    "tok_ds = ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)\n",
    "tok_ds.set_format(type='torch')\n",
    "\n",
    "training_args = TrainingArguments(output_dir='./fine_tuned_jokes',\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  num_train_epochs=3,\n",
    "                                  save_steps=500)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=tok_ds)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "## Try it (exécution)\n",
    "- Exécutez les cellules dans l'ordre: installation -> génération -> filtrage.\n",
    "- Si vous avez des modèles locaux pour la génération, modifiez `model_name_or_path` pour pointer vers le répertoire du modèle.\n",
    "- Vérifiez que `datasets/shortjokes.csv` contient une colonne `text` ou adaptez le nom de colonne dans le squelette.\n",
    "\n",
    "---\n",
    "Si vous voulez, je peux: fournir un script standalone, préparer un fine-tuning complet avec `accelerate`, ou générer un petit jeu d'évaluation pour mesurer la qualité des blagues filtrées."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
